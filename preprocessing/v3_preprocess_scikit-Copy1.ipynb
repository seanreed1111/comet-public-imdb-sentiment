{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from download_imdb import maybe_download_imdb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "['and', 'document', 'first', 'here', 'is', 'one', 'or', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "vec1 = CountVectorizer(ngram_range=(1, 1), \n",
    "                      strip_accents='unicode')\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'Here is the first or the third?'\n",
    "]\n",
    "new_corp = vec1.fit_transform(corpus)\n",
    "print(type(new_corp))\n",
    "new_corp.toarray()\n",
    "print(vec1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_string = \"\"\"\n",
    "'s\n",
    "a\n",
    "able\n",
    "about\n",
    "above\n",
    "abst\n",
    "accordance\n",
    "according\n",
    "accordingly\n",
    "across\n",
    "act\n",
    "actually\n",
    "added\n",
    "adj\n",
    "affected\n",
    "affecting\n",
    "affects\n",
    "after\n",
    "afterwards\n",
    "again\n",
    "against\n",
    "ah\n",
    "all\n",
    "almost\n",
    "alone\n",
    "along\n",
    "already\n",
    "also\n",
    "although\n",
    "always\n",
    "am\n",
    "among\n",
    "amongst\n",
    "an\n",
    "and\n",
    "announce\n",
    "another\n",
    "any\n",
    "anybody\n",
    "anyhow\n",
    "anymore\n",
    "anyone\n",
    "anything\n",
    "anyway\n",
    "anyways\n",
    "anywhere\n",
    "apparently\n",
    "approximately\n",
    "are\n",
    "aren\n",
    "arent\n",
    "arise\n",
    "around\n",
    "as\n",
    "aside\n",
    "ask\n",
    "asking\n",
    "at\n",
    "auth\n",
    "available\n",
    "away\n",
    "awfully\n",
    "back\n",
    "be\n",
    "became\n",
    "because\n",
    "become\n",
    "becomes\n",
    "becoming\n",
    "been\n",
    "before\n",
    "beforehand\n",
    "begin\n",
    "beginning\n",
    "beginnings\n",
    "begins\n",
    "behind\n",
    "being\n",
    "believe\n",
    "below\n",
    "beside\n",
    "besides\n",
    "between\n",
    "beyond\n",
    "biol\n",
    "both\n",
    "brief\n",
    "briefly\n",
    "but\n",
    "by\n",
    "came\n",
    "can\n",
    "cannot\n",
    "can't\n",
    "cause\n",
    "causes\n",
    "certain\n",
    "certainly\n",
    "co\n",
    "com\n",
    "come\n",
    "comes\n",
    "contain\n",
    "containing\n",
    "contains\n",
    "could\n",
    "couldnt\n",
    "date\n",
    "did\n",
    "didn't\n",
    "different\n",
    "do\n",
    "does\n",
    "doesn't\n",
    "doing\n",
    "done\n",
    "don't\n",
    "down\n",
    "downwards\n",
    "due\n",
    "during\n",
    "each\n",
    "ed\n",
    "edu\n",
    "effect\n",
    "eg\n",
    "eight\n",
    "eighty\n",
    "either\n",
    "else\n",
    "elsewhere\n",
    "end\n",
    "ending\n",
    "enough\n",
    "especially\n",
    "et\n",
    "et-al\n",
    "etc\n",
    "even\n",
    "ever\n",
    "every\n",
    "everybody\n",
    "everyone\n",
    "everything\n",
    "everywhere\n",
    "ex\n",
    "except\n",
    "far\n",
    "few\n",
    "fifth\n",
    "first\n",
    "five\n",
    "fix\n",
    "followed\n",
    "following\n",
    "follows\n",
    "for\n",
    "former\n",
    "formerly\n",
    "forth\n",
    "found\n",
    "four\n",
    "from\n",
    "further\n",
    "furthermore\n",
    "gave\n",
    "get\n",
    "gets\n",
    "getting\n",
    "give\n",
    "given\n",
    "gives\n",
    "giving\n",
    "go\n",
    "goes\n",
    "gone\n",
    "got\n",
    "gotten\n",
    "had\n",
    "happens\n",
    "hardly\n",
    "has\n",
    "hasn't\n",
    "have\n",
    "haven't\n",
    "having\n",
    "he\n",
    "hed\n",
    "hence\n",
    "her\n",
    "here\n",
    "hereafter\n",
    "hereby\n",
    "herein\n",
    "heres\n",
    "hereupon\n",
    "hers\n",
    "herself\n",
    "hes\n",
    "hi\n",
    "hid\n",
    "him\n",
    "himself\n",
    "his\n",
    "hither\n",
    "home\n",
    "how\n",
    "howbeit\n",
    "however\n",
    "hundred\n",
    "i\n",
    "id\n",
    "ie\n",
    "if\n",
    "i'll\n",
    "im\n",
    "immediate\n",
    "immediately\n",
    "importance\n",
    "important\n",
    "in\n",
    "inc\n",
    "indeed\n",
    "index\n",
    "information\n",
    "instead\n",
    "into\n",
    "invention\n",
    "inward\n",
    "is\n",
    "isn't\n",
    "it\n",
    "itd\n",
    "it'll\n",
    "its\n",
    "itself\n",
    "i've\n",
    "just\n",
    "keep\n",
    "keeps\n",
    "kept\n",
    "kg\n",
    "km\n",
    "know\n",
    "known\n",
    "knows\n",
    "largely\n",
    "last\n",
    "lately\n",
    "later\n",
    "latter\n",
    "latterly\n",
    "least\n",
    "less\n",
    "lest\n",
    "let\n",
    "lets\n",
    "like\n",
    "liked\n",
    "likely\n",
    "line\n",
    "little\n",
    "'ll\n",
    "look\n",
    "looking\n",
    "looks\n",
    "ltd\n",
    "made\n",
    "mainly\n",
    "make\n",
    "makes\n",
    "many\n",
    "may\n",
    "maybe\n",
    "me\n",
    "mean\n",
    "means\n",
    "meantime\n",
    "meanwhile\n",
    "merely\n",
    "mg\n",
    "might\n",
    "million\n",
    "miss\n",
    "ml\n",
    "more\n",
    "moreover\n",
    "most\n",
    "mostly\n",
    "mr\n",
    "mrs\n",
    "much\n",
    "mug\n",
    "must\n",
    "my\n",
    "myself\n",
    "name\n",
    "namely\n",
    "nay\n",
    "near\n",
    "nearly\n",
    "necessarily\n",
    "necessary\n",
    "need\n",
    "needs\n",
    "neither\n",
    "never\n",
    "nevertheless\n",
    "new\n",
    "next\n",
    "nine\n",
    "ninety\n",
    "no\n",
    "nobody\n",
    "non\n",
    "none\n",
    "nonetheless\n",
    "noone\n",
    "nor\n",
    "normally\n",
    "nos\n",
    "not\n",
    "n't\n",
    "noted\n",
    "nothing\n",
    "now\n",
    "nowhere\n",
    "obtain\n",
    "obtained\n",
    "obviously\n",
    "of\n",
    "off\n",
    "often\n",
    "oh\n",
    "ok\n",
    "okay\n",
    "old\n",
    "omitted\n",
    "on\n",
    "once\n",
    "one\n",
    "ones\n",
    "only\n",
    "onto\n",
    "or\n",
    "ord\n",
    "other\n",
    "others\n",
    "otherwise\n",
    "ought\n",
    "our\n",
    "ours\n",
    "ourselves\n",
    "out\n",
    "outside\n",
    "over\n",
    "overall\n",
    "owing\n",
    "own\n",
    "p\n",
    "page\n",
    "pages\n",
    "part\n",
    "particular\n",
    "particularly\n",
    "past\n",
    "per\n",
    "perhaps\n",
    "placed\n",
    "please\n",
    "plus\n",
    "poorly\n",
    "possible\n",
    "possibly\n",
    "potentially\n",
    "predominantly\n",
    "present\n",
    "previously\n",
    "primarily\n",
    "probably\n",
    "promptly\n",
    "proud\n",
    "provides\n",
    "put\n",
    "que\n",
    "quickly\n",
    "quite\n",
    "ran\n",
    "rather\n",
    "readily\n",
    "really\n",
    "recent\n",
    "recently\n",
    "ref\n",
    "refs\n",
    "regarding\n",
    "regardless\n",
    "regards\n",
    "related\n",
    "relatively\n",
    "research\n",
    "respectively\n",
    "resulted\n",
    "resulting\n",
    "results\n",
    "right\n",
    "run\n",
    "said\n",
    "same\n",
    "saw\n",
    "say\n",
    "saying\n",
    "says\n",
    "sec\n",
    "section\n",
    "see\n",
    "seeing\n",
    "seem\n",
    "seemed\n",
    "seeming\n",
    "seems\n",
    "seen\n",
    "self\n",
    "selves\n",
    "sent\n",
    "seven\n",
    "several\n",
    "shall\n",
    "she\n",
    "shed\n",
    "she'll\n",
    "shes\n",
    "should\n",
    "shouldn't\n",
    "show\n",
    "showed\n",
    "shown\n",
    "showns\n",
    "shows\n",
    "significant\n",
    "significantly\n",
    "similar\n",
    "similarly\n",
    "since\n",
    "six\n",
    "slightly\n",
    "so\n",
    "some\n",
    "somebody\n",
    "somehow\n",
    "someone\n",
    "somethan\n",
    "something\n",
    "sometime\n",
    "sometimes\n",
    "somewhat\n",
    "somewhere\n",
    "soon\n",
    "sorry\n",
    "specifically\n",
    "specified\n",
    "specify\n",
    "specifying\n",
    "still\n",
    "stop\n",
    "strongly\n",
    "sub\n",
    "substantially\n",
    "successfully\n",
    "such\n",
    "sufficiently\n",
    "suggest\n",
    "sup\n",
    "sure\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stop_string.strip().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'POSITIVE':True} {'POSITIVE':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_y(y_raw):\n",
    "    \"\"\"\n",
    "    Strips out True and False values from the dicts in y_raw. Makes and returns an np array of dtype=int from the values\n",
    "    \"\"\"\n",
    "    y_clean =np.array([item[\"POSITIVE\"] for item in y_raw], dtype=int)\n",
    "    return y_clean\n",
    "\n",
    "\n",
    "filename = 'imdb_thinc_data.pickle'\n",
    "filepath = os.path.join('..','data','thinc',filename)\n",
    "_, _, X_test_raw, y_test_raw =maybe_download_imdb(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"No Fireworks Despite Violent Action.\\n\\n\\n\\nScience fiction films that reflect quality are scarce indeed, largely because transposal of imaginative themes from the genre to the screen too often falls short of effective execution as a result of insufficient funding or inadequate invention, and unfortunately for its producers, this work is lacking on both counts, woefully so in the case of the latter. With essentially no budget with which to operate, it is a grave mistake to attempt the depiction of such a gamut of events as those within this scenario and, in particular, special effects of space opera warfare which appear only clownish, while seeds from the scriptors' imagination lie fallow due to some of the most fatuous misunderstanding of basic scientific principles to be found. Among these are frequent firing of weapons within a sealed environment, and a wayward law of gravity which enables freedom of movement of cast members while inanimate objects float weightlessly, but it is easier to accept these than it is to pretend that any of the episodes have a basis in plausibility. The plot involves an escape of life sentenced prisoners from a space station penal colony to a waste landfill upon our moon and their various attempts to obtain passage back to Earth, with some few capable players present who are execrably directed by first-timer Paolo Mazzucato, whose production team wastes effort upon such as holographic pornography while ignoring a pressing and basic requirement for the creation of states of suspense and of impetus.\\n\\n\\n\\n\",\n",
       " 'This is the second British Rank film to adapt the stories of Sommerset Maugham to film. All but one story from \\'Quartet\\' does not travel well into the contempory era; and the actors speech is decidedly \"clipped\", as only British pre-1950\\'s actors delivery can be. In anycase \\'Trio\\' seems tighter and more filmic than the first film adaptation.\\n\\n\\n\\nOne of the problems these two films can\\'t overcome is that their source material was written 25-30 years prior to the films. Consequently, by the 1950\\'s Maughm\\'s (pre-war) popularist \"small morality\" storyteling seemed rather quaint, if not downright coy.',\n",
       " 'This is one of the most god-awful movies ever. Shaq better just stick to basketball. This movie took away apart of my life I will never have back. I will make fun of this movie until I die, and then some. It is so horrible it is not even funny. MST3000 would have a blast with this one.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array_raw= [X_test_raw[5],X_test_raw[6],X_test_raw[7]]\n",
    "X_array_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use this as supplied tokenizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "vec = CountVectorizer(ngram_range=(1, 1), \n",
    "                      stop_words=stops, \n",
    "                      strip_accents='unicode',\n",
    "                     token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                     tokenizer=LemmaTokenizer())\n",
    "\n",
    "counts = vec.fit_transform(X_array_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
